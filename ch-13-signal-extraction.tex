In this chapter we describe the statistical methodology used to perform signal extraction. 

\section{Model building and parameter estimation}
In the frequentist interpretation of probability, an experiment measuring an observable can be repeated, resulting in different values of the observable, e.g. the invariant mass of a candidate Higgs boson in a search for the Higgs \cite{2011-Statistics-Cranmer}. The ensemble of values of the observable $x$ gives rise to the probability density function (PDF) $f(x)$, which has the important property that it is normalized to unity:
\begin{equation*}
    \int f(x) \, dx = 1 \,.
\end{equation*}
A parametric family of PDFs
\begin{equation*}
    f(x|\alpha) \, ,
\end{equation*}
read ``$f$ of $x$ given $\alpha$", is referred to as a probability model or model. The parameters $\alpha$ typically represent parameters of the theory or an unknown property of the detector's response. The parameters are not frequentist in nature, unlike $x$. Out of all the parameters, typically only a few are of interest, and are called the parameters of interest (POI), labeled $\mu$ here. The remaining are referred to as nuisance parameters (NP) \cite{2011-Statistics-Cranmer} and are labeled $\boldsymbol{\theta}$.

$f(x)$ is the probability density for the observable in one event and we wish to describe the probability density for a dataset with many events, $\mathcal{D} = \{x_1, ..., x_n\}$, called the total probability model $\boldsymbol{f}$. For instance, if we also have a prediction for the total number of events expected, called $\nu$, we also account for the overall Poisson probability for observing $n$ events given $\nu$ expected:
\begin{equation}
    \boldsymbol{f}(\mathcal{D}|\nu, \alpha) = \text{Poisson}(n|\nu) \prod_{e=1}^{n} f(x_e | \alpha)
\end{equation}

The likelihood function $L(\alpha)$ is numerically equivalent to $f(x|\alpha)$ for fixed $x$, or $\boldsymbol{f}(\mathcal{D}|\alpha)$ with $\mathcal{D}$ fixed \cite{2011-Statistics-Cranmer}. The likelihood function is not a probability density for $\alpha$ and is not normalized to unity:
\begin{equation*}
    \int L(\alpha) \, d(\alpha) \neq 1 \, .
\end{equation*}
i.e. the likelihood function is the value of $f$ as a function of $\alpha$ given a fixed value of $x$.

To estimate the parameter $\alpha$ we use an estimator, which is a function of the data. Take for example the measurement of data distributed according to a Gaussian probability density $f(x| \mu,\sigma) = \text{Gauss}(x|\mu,\sigma)$. One possible estimator of the mean $\mu$, is the mean of the measured data points $\bar{x} = \sum_{i = 1}^{n} x_i / n$ \cite{2011-Statistics-Cranmer}. 

A commonly used estimator in physics is the maximum likelihood estimator (MLE), defined as the value $\alpha$ which maximizes the likelihood function $L(\alpha)$. This value, labeled $\hat{\alpha}$, also maximizes $\ln L(\alpha)$ and minimizes $-\ln L(\alpha)$. By convention the $-\ln L(\alpha)$ is minimized, in a process called ``fitting", and the maximum likelihood estimate is called the ``best fit value". 


% Profile likelihood ratios

% How to get a Confidence Level 

% Asimov dataset

\section{Hypothesis testing}
In this section we next introduce concepts related to hypothesis testing such as the test statistic constructed from the ratio of likelihood functions.

The objective of a likelihood analysis is to distinguish different models representing the various hypotheses, and determine the one that best explains the experimental outcome. In a search for new physics, a signal is additive on top of the background. The background-only hypothesis is the null hypothesis, and the signal-plus-background hypothesis is the alternative. 

As a simple example, take the $p$-value test, for an experiment where we count events in the signal region, $n_{SR}$, and expect $\nu_B$ background events and $\nu_S$ events from the signal \cite{2011-Statistics-Cranmer}. Then 
\begin{enumerate}
    \item The null hypothesis ($H_0$), i.e. the background-only hypothesis in this experiment, with the probability modeled by Poisson($n_{SR}|\nu_B$).
    \item The alternate hypothesis ($H_1$), i.e. signal-plus-background hypothesis, with the probability modeled by Poisson($n_{SR}|(\nu_B + \nu_S)$).
\end{enumerate}
The compatability of the observed data $\nu^0_{SR}$ and the null hypothesis, is quantified as the probability that the background-only hypothesis would produce at least as many events as was observed. This probability is the $p$-value: 
\begin{equation}
    p = \sum_{n = n^0_{SR}}^{\infty} \text{Poisson}(n | \nu_B) \, .
\end{equation}
If the $p$-value is very small, we might reject the null hypothesis. The $p$-value is not the probability of the null hypothesis given the data; rather, it expresses the probability that data with a certain property was obtained, assuming the null hypothesis \cite{2011-Statistics-Cranmer}.

The $p$-value is an example of a test statistic $T$, which maps the data to a single real number. The Neyman-Pearson lemma states that out of the infinite possibilities of choices of test statistic, the uniformly most powerful test statistic is the likelihood ratio $T_{NP}$ \cite{2011-Statistics-Cranmer}:

\begin{equation}
    T_{NP}(\mathcal{D}) = \frac{L(\mathcal{D} | {H_1})}{L(\mathcal{D}|{H_0})}
\end{equation}
To reiterate, the test statistic $T$ is a real-valued function of the data, implying that a particular probability model $\boldsymbol{f}(\mathcal{D}|\boldsymbol{\alpha})$ implies a distribution of the test statistic, $f(T|\boldsymbol{\alpha})$, which depends on the value of $\alpha$. With this distribution in hand, the $p$-value can be evaluated in the followng equivalent formulations:
\begin{align}
    p(\boldsymbol{\alpha}) &= \int_{T_0}^{\infty} f(T|\boldsymbol{\alpha}) \, dT  \\
              &= \int \boldsymbol{f}(\mathcal{D} | \boldsymbol{\alpha}) \, \theta(T(\mathcal{D}) - T_0) \, d\mathcal{D} \\
              &= P(T \geq T_0 | \boldsymbol{\alpha})
\end{align}
where $T_0$ is the value of $T$ based on the observed data, and $\theta()$ is the Heaviside function. The size of the test is conventionally chosen to be 10\%, 5\%, or 1\%. As the $p$-value depends on $\boldsymbol{\alpha}$ (both the POI and NP), the null hypothesis should not be rejected if the $p$-value is larger than the size of the test for any value of the nuisance parameters.

\section{Confidence intervals}
In an example of the measurement of the Standard Model Higgs boson, $\boldsymbol{\alpha}_{\text{POI}} = (\sigma/ \sigma_{SM}, M_H)$, with $\sigma/\sigma_{SM}$ is the ratio of the production cross-section for Higgs with respect to its value in the SM, and $M_H$ is the unknown mass of the Higgs, values of these parameters outside specific bounds are said to be ``excluded at the 95\% confidence level''. These allowed regions are called confidence levels or confidence regions, and the parameter values outside of them are considered excluded \cite{2011-Statistics-Cranmer}. A 95\% confidence interval does not mean that there is a 95\% chance that the true value of the parameter is inside the interval. Rather, a 95\% confidence interval covers the true value 95\% of the time (even though we do not know the true value). 

To construct a confidence interval for a parameter $\alpha$, the Neyman Construction is used to invert a series of hypothesis tests; i.e. for each possible value of $\alpha$, the null hypothesis is treated as $\alpha$, and we perform a hypothesis test based on a test statistic. To construct a 95\% confidence interval, we construct a series of hypothesis tests with size of 5\%. The confidence interval $I(\mathcal{D})$ is constructed by taking the set of parameter values $\boldsymbol{\alpha}$ where the null hypothesis is accepted:
\begin{equation}
    I(\mathcal{D}) = \{ \boldsymbol{\alpha} | P(T(\mathcal{D}) > k_\alpha | \boldsymbol{\alpha}) < \alpha \} \, ,
\end{equation} 
where $T(\mathcal{D})$ is the test statistic, and the last $\alpha$ (not bolded) and the subscript $k_\alpha$ refer to the size of the test. A schematic of the Neyman construction is shown in Fig. \ref{fig:neyman-construction}. In a more generalized case, the $x$-axis is the test statistic $T$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=15cm]{figures/ch-13-signal-extraction/schematic_neyman_construction.png}
    \caption[Schematic of the Neyman construction for confidence intervals.]{Schematic of the Neyman construction for confidence intervals \cite{2011-Statistics-Cranmer}. For each value of $\theta$, we find a region in $x$ where $\int f(x|\theta) dx$ satisfies the size of the test (\textit{blue}). These regions form a confidence belt (\textit{green}). The intersection of the observation $x_0$ (\textit{red}) with the confidence belt defines the confidence interval $[\theta_1, \theta_2]$ \cite{2011-Statistics-Cranmer}.} 
    \label{fig:neyman-construction}
\end{figure}

\section{Profiling}

In this section we describe a frequentist statistical procedure based on the profile likelihood ratio test statistic, which is implemented using asymptotic distributions.

With a multi-parameter likelihood function $L(\boldsymbol{\alpha})$, the the maximum likelihood of one specific parameter $\alpha_p$ with other parameters $\boldsymbol{\alpha}_o$ fixed, is called the conditional maximum likelihood estimate and is denoted $\doublehat{\alpha}_p(\boldsymbol{\alpha_0})$.
The process of choosing specific values of the nuisance parameters for a given value of $\mu$, $\mathcal{D}_{\text{simulated}}$, and value of global observables $\mathcal{G}$ is called profiling. From the full list of parameters $\boldsymbol{\alpha}$, we denote the parameter of interest $\mu$, and the nuisance parameters $\boldsymbol{\theta}$.

We construct the profile likelihood ratio,
\begin{equation}
    \lambda(\mu) = \frac{L(\mu, \doublehat{\boldsymbol{\theta}}(\mu))}{L({\mu, \hat{\boldsymbol{\theta}}})}
\end{equation}
which depends explicitly on the parameter of interest $\mu$, implicitly on the data $\mathcal{D}_{\text{sim}}$ and global observables $\mathcal{G}$, and is independent of the nuisance parameters $\boldsymbol{\theta}$, which have been eliminated in profiling \cite{2011-Statistics-Cranmer}.

The main conceptual reason for constructing the test statistic from the profile likelihood ratio is that asymptotically (i.e. for measurements with many events) the distribution of the profile likelihood ratio $\lambda(\mu = \mu_{\text{true}})$ is independent of the values of the nuisance parameters \cite{2011-Statistics-Cranmer}. 

The following $p$-value is used to quantify the consistency with the hypothesis of a signal strength of $\mu$:
\begin{equation}
    p_\mu = \int_{\tilde{q_{\mu, \text{obs}}}}^{\infty} f(\tilde{q}_\mu | \mu, \doublehat{\boldsymbol{\theta}}(\mu, \text{obs})) \, d\tilde{q}_\mu
\end{equation}

% TODO: elaborate (section 3.3 of Cranmer https://cds.cern.ch/record/2004587/files/267-308%20Cranmer.pdf)

\section{Modified frequentist method: $CL_{S}$}
In the

\section{Combining of multiple channels}
Analyses frequently have multiple search channels which need to be combined \cite{2011-Cowan-et-al}. We assume that there is one strength parameter $\mu$ that is the same for all channels. Each channel $i$ has a likelihood function $L_{i} (\mu, \boldsymbol{\theta}_i)$, where $\boldsymbol{\theta_i}$ represents the set of nuisance parameters for the $i$th channel, some of which may be common between the channels. If the channels are statistically independent, the full likelihood function is the product
\begin{equation}
    L(\mu, \boldsymbol{\theta}) = \prod_{i} L_i (\mu, \boldsymbol{\theta_i})
\end{equation}
where $\boldsymbol{\theta}$ is the complete set of all nuisance parameters. The profile likelihood ratio is 
\begin{equation}
    \lambda(\mu) = \frac{ \prod_i L_i(\mu, \hat{\hat{\boldsymbol{\theta}}}_i) }{ \prod_i L_i(\mu, \hat{\boldsymbol{\theta}}_i)}
\end{equation}

% TODO: finish this section once I also define what a profile likelihood ratio is